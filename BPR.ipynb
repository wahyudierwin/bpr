{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(path = os.getcwd())\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from tqdm import trange\n",
    "from subprocess import call\n",
    "from itertools import islice\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix, dok_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimension: \n",
      " (100000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dir = 'ml-100k'\n",
    "file_path = os.path.join(file_dir, 'u.data')\n",
    "if not os.path.isdir(file_dir):\n",
    "    call(['curl', '-O', 'http://files.grouplens.org/datasets/movielens/' + file_dir + '.zip'])\n",
    "    call(['unzip', file_dir + '.zip'])\n",
    "\n",
    "# we will not be using the timestamp column\n",
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv(file_path, sep = '\\t', names = names)\n",
    "print('data dimension: \\n', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(data, users_col, items_col, ratings_col, threshold = None):\n",
    "    \"\"\"\n",
    "    creates the sparse user-item interaction matrix,\n",
    "    if the data is not in the format where the interaction only\n",
    "    contains the positive items (indicated by 1), then use the \n",
    "    threshold parameter to determine which items are considered positive\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        implicit rating data\n",
    "\n",
    "    users_col : str\n",
    "        user column name\n",
    "\n",
    "    items_col : str\n",
    "        item column name\n",
    "    \n",
    "    ratings_col : str\n",
    "        implicit rating column name\n",
    "\n",
    "    threshold : int, default None\n",
    "        threshold to determine whether the user-item pair is \n",
    "        a positive feedback\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "        user/item ratings matrix\n",
    "\n",
    "    data : DataFrame\n",
    "        implict rating data that retains only the positive feedback\n",
    "        (if specified to do so)\n",
    "    \"\"\"\n",
    "    if threshold is not None:\n",
    "        data = data[data[ratings_col] >= threshold]\n",
    "        data[ratings_col] = 1\n",
    "    \n",
    "    for col in (items_col, users_col, ratings_col):\n",
    "        data[col] = data[col].astype('category')\n",
    "\n",
    "    ratings = csr_matrix((data[ratings_col],\n",
    "                          (data[users_col].cat.codes, data[items_col].cat.codes)))\n",
    "    ratings.eliminate_zeros()\n",
    "    return ratings, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data, users_col, items_col, threshold = 5):\n",
    "    \"\"\"\n",
    "    choose data with user_id whose item count is\n",
    "    greater than threshold\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        implicit rating data\n",
    "\n",
    "    users_col : str\n",
    "        user column name\n",
    "\n",
    "    items_col : str\n",
    "        item column name\n",
    "    \n",
    "    threshold : int, default 5\n",
    "        threshold to determine whether the user is chosen as training data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_filter : DataFrame\n",
    "        Filtered data where a user is eligible to be in training data\n",
    "    \"\"\"\n",
    "    \n",
    "    data_count = data.groupby(users_col).count()[[items_col]] \\\n",
    "        .reset_index().rename(columns={items_col: 'item_count'})\n",
    "    \n",
    "    data_filter = data \\\n",
    "        .merge(data_count[data_count['item_count'] >= threshold][[users_col]], \\\n",
    "               on=users_col)\n",
    "    \n",
    "    return data_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = filter_data(df, 'user_id', 'item_id', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, df_train = create_matrix(df_train, 'user_id', 'item_id', 'rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<568x1681 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 88471 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>566</td>\n",
       "      <td>5</td>\n",
       "      <td>879023663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>186</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>879023607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186</td>\n",
       "      <td>148</td>\n",
       "      <td>4</td>\n",
       "      <td>891719774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>186</td>\n",
       "      <td>263</td>\n",
       "      <td>3</td>\n",
       "      <td>879023571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88466</th>\n",
       "      <td>930</td>\n",
       "      <td>244</td>\n",
       "      <td>4</td>\n",
       "      <td>879535392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88467</th>\n",
       "      <td>930</td>\n",
       "      <td>190</td>\n",
       "      <td>4</td>\n",
       "      <td>879535492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88468</th>\n",
       "      <td>930</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>879534925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88469</th>\n",
       "      <td>930</td>\n",
       "      <td>535</td>\n",
       "      <td>4</td>\n",
       "      <td>879535392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88470</th>\n",
       "      <td>930</td>\n",
       "      <td>410</td>\n",
       "      <td>3</td>\n",
       "      <td>879534973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88471 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id item_id rating  timestamp\n",
       "0         186     302      3  891717742\n",
       "1         186     566      5  879023663\n",
       "2         186     250      1  879023607\n",
       "3         186     148      4  891719774\n",
       "4         186     263      3  879023571\n",
       "...       ...     ...    ...        ...\n",
       "88466     930     244      4  879535392\n",
       "88467     930     190      4  879535492\n",
       "88468     930      16      1  879534925\n",
       "88469     930     535      4  879535392\n",
       "88470     930     410      3  879534973\n",
       "\n",
       "[88471 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPR:\n",
    "    \"\"\"\n",
    "    Bayesian Personalized Ranking (BPR) for implicit feedback data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float, default 0.01\n",
    "        learning rate for gradient descent\n",
    "\n",
    "    n_factors : int, default 20\n",
    "        Number/dimension of user and item latent factors\n",
    "\n",
    "    n_iters : int, default 15\n",
    "        Number of iterations to train the algorithm\n",
    "        \n",
    "    batch_size : int, default 1000\n",
    "        batch size for batch gradient descent, the original paper\n",
    "        uses stochastic gradient descent (i.e., batch size of 1),\n",
    "        but this can make the training unstable (very sensitive to\n",
    "        learning rate)\n",
    "\n",
    "    reg : int, default 0.01\n",
    "        Regularization term for the user and item latent factors\n",
    "\n",
    "    seed : int, default 1234\n",
    "        Seed for the randomly initialized user, item latent factors\n",
    "\n",
    "    verbose : bool, default True\n",
    "        Whether to print progress bar while training\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    user_factors : 2d ndarray, shape [n_users, n_factors]\n",
    "        User latent factors learnt\n",
    "\n",
    "    item_factors : 2d ndarray, shape [n_items, n_factors]\n",
    "        Item latent factors learnt\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    S. Rendle, C. Freudenthaler, Z. Gantner, L. Schmidt-Thieme \n",
    "    Bayesian Personalized Ranking from Implicit Feedback\n",
    "    - https://arxiv.org/abs/1205.2618\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate = 0.01, n_factors = 15, n_iters = 10, \n",
    "                 batch_size = 1000, reg = 0.01, seed = 1234, verbose = True):\n",
    "        self.reg = reg\n",
    "        self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.n_iters = n_iters\n",
    "        self.n_factors = n_factors\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # to avoid re-computation at predict\n",
    "        self._prediction = None\n",
    "        \n",
    "    def fit(self, ratings):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "            sparse matrix of user-item interactions\n",
    "        \"\"\"\n",
    "        indptr = ratings.indptr\n",
    "        indices = ratings.indices\n",
    "        n_users, n_items = ratings.shape\n",
    "        \n",
    "        # ensure batch size makes sense, since the algorithm involves\n",
    "        # for each step randomly sample a user, thus the batch size\n",
    "        # should be smaller than the total number of users or else\n",
    "        # we would be sampling the user with replacement\n",
    "        batch_size = self.batch_size\n",
    "        if n_users < batch_size:\n",
    "            batch_size = n_users\n",
    "            sys.stderr.write('WARNING: Batch size is greater than number of users,'\n",
    "                             'switching to a batch size of {}\\n'.format(n_users))\n",
    "\n",
    "        batch_iters = n_users // batch_size\n",
    "        \n",
    "        # initialize random weights\n",
    "        rstate = np.random.RandomState(self.seed)\n",
    "        self.user_factors = rstate.normal(size = (n_users, self.n_factors))\n",
    "        self.item_factors = rstate.normal(size = (n_items, self.n_factors))\n",
    "        \n",
    "        # progress bar for training iteration if verbose is turned on\n",
    "        loop = range(self.n_iters)\n",
    "        if self.verbose:\n",
    "            loop = trange(self.n_iters, desc = self.__class__.__name__)\n",
    "        \n",
    "        for _ in loop:\n",
    "            for _ in range(batch_iters):\n",
    "                sampled = self._sample(n_users, n_items, indices, indptr)\n",
    "                sampled_users, sampled_pos_items, sampled_neg_items = sampled\n",
    "                self._update(sampled_users, sampled_pos_items, sampled_neg_items)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _sample(self, n_users, n_items, indices, indptr):\n",
    "        \"\"\"sample batches of random triplets u, i, j\"\"\"\n",
    "        sampled_pos_items = np.zeros(self.batch_size, dtype = np.int)\n",
    "        sampled_neg_items = np.zeros(self.batch_size, dtype = np.int)\n",
    "        sampled_users = np.random.choice(\n",
    "            n_users, size = self.batch_size, replace = False)\n",
    "\n",
    "        for idx, user in enumerate(sampled_users):\n",
    "            pos_items = indices[indptr[user]:indptr[user + 1]]\n",
    "            pos_item = np.random.choice(pos_items)\n",
    "            neg_item = np.random.choice(n_items)\n",
    "            while neg_item in pos_items:\n",
    "                neg_item = np.random.choice(n_items)\n",
    "\n",
    "            sampled_pos_items[idx] = pos_item\n",
    "            sampled_neg_items[idx] = neg_item\n",
    "\n",
    "        return sampled_users, sampled_pos_items, sampled_neg_items\n",
    "                \n",
    "    def _update(self, u, i, j):\n",
    "        \"\"\"\n",
    "        update according to the bootstrapped user u, \n",
    "        positive item i and negative item j\n",
    "        \"\"\"\n",
    "        user_u = self.user_factors[u]\n",
    "        item_i = self.item_factors[i]\n",
    "        item_j = self.item_factors[j]\n",
    "        \n",
    "        # decompose the estimator, compute the difference between\n",
    "        # the score of the positive items and negative items; a\n",
    "        # naive implementation might look like the following:\n",
    "        # r_ui = np.diag(user_u.dot(item_i.T))\n",
    "        # r_uj = np.diag(user_u.dot(item_j.T))\n",
    "        # r_uij = r_ui - r_uj\n",
    "        \n",
    "        # however, we can do better, so\n",
    "        # for batch dot product, instead of doing the dot product\n",
    "        # then only extract the diagonal element (which is the value\n",
    "        # of that current batch), we perform a hadamard product, \n",
    "        # i.e. matrix element-wise product then do a sum along the column will\n",
    "        # be more efficient since it's less operations\n",
    "        # http://people.revoledu.com/kardi/tutorial/LinearAlgebra/HadamardProduct.html\n",
    "        # r_ui = np.sum(user_u * item_i, axis = 1)\n",
    "        #\n",
    "        # then we can achieve another speedup by doing the difference\n",
    "        # on the positive and negative item up front instead of computing\n",
    "        # r_ui and r_uj separately, these two idea will speed up the operations\n",
    "        # from 1:14 down to 0.36\n",
    "        r_uij = np.sum(user_u * (item_i - item_j), axis = 1)\n",
    "        sigmoid = np.exp(-r_uij) / (1.0 + np.exp(-r_uij))\n",
    "        \n",
    "        # repeat the 1 dimension sigmoid n_factors times so\n",
    "        # the dimension will match when doing the update\n",
    "        sigmoid_tiled = np.tile(sigmoid, (self.n_factors, 1)).T\n",
    "\n",
    "        # update using gradient descent\n",
    "        grad_u = sigmoid_tiled * (item_j - item_i) + self.reg * user_u\n",
    "        grad_i = sigmoid_tiled * -user_u + self.reg * item_i\n",
    "        grad_j = sigmoid_tiled * user_u + self.reg * item_j\n",
    "        self.user_factors[u] -= self.learning_rate * grad_u\n",
    "        self.item_factors[i] -= self.learning_rate * grad_i\n",
    "        self.item_factors[j] -= self.learning_rate * grad_j\n",
    "        return self\n",
    "\n",
    "    def predict(self):\n",
    "        \"\"\"\n",
    "        Obtain the predicted ratings for every users and items\n",
    "        by doing a dot product of the learnt user and item vectors.\n",
    "        The result will be cached to avoid re-computing it every time\n",
    "        we call predict, thus there will only be an overhead the first\n",
    "        time we call it. Note, ideally you probably don't need to compute\n",
    "        this as it returns a dense matrix and may take up huge amounts of\n",
    "        memory for large datasets\n",
    "        \"\"\"\n",
    "        if self._prediction is None:\n",
    "            self._prediction = self.user_factors.dot(self.item_factors.T)\n",
    "\n",
    "        return self._prediction\n",
    "\n",
    "    def _predict_user(self, user):\n",
    "        \"\"\"\n",
    "        returns the predicted ratings for the specified user,\n",
    "        this is mainly used in computing evaluation metric\n",
    "        \"\"\"\n",
    "        user_pred = self.user_factors[user].dot(self.item_factors.T)\n",
    "        return user_pred\n",
    "\n",
    "    def recommend(self, ratings, N = 5):\n",
    "        \"\"\"\n",
    "        Returns the top N ranked items for given user id,\n",
    "        excluding the ones that the user already liked\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ratings : scipy sparse csr_matrix, shape [n_users, n_items]\n",
    "            sparse matrix of user-item interactions \n",
    "        \n",
    "        N : int, default 5\n",
    "            top-N similar items' N\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        recommendation : 2d ndarray, shape [number of users, N]\n",
    "            each row is the top-N ranked item for each query user\n",
    "        \"\"\"\n",
    "        n_users = ratings.shape[0]\n",
    "        recommendation = np.zeros((n_users, N), dtype = np.uint32)\n",
    "        for user in range(n_users):\n",
    "            top_n = self._recommend_user(ratings, user, N)\n",
    "            recommendation[user] = top_n\n",
    "\n",
    "        return recommendation\n",
    "\n",
    "    def _recommend_user(self, ratings, user, N):\n",
    "        \"\"\"the top-N ranked items for a given user\"\"\"\n",
    "        scores = self._predict_user(user)\n",
    "\n",
    "        # compute the top N items, removing the items that the user already liked\n",
    "        # from the result and ensure that we don't get out of bounds error when \n",
    "        # we ask for more recommendations than that are available\n",
    "        liked = set(ratings[user].indices)\n",
    "        count = N + len(liked)\n",
    "        if count < scores.shape[0]:\n",
    "\n",
    "            # when trying to obtain the top-N indices from the score,\n",
    "            # using argpartition to retrieve the top-N indices in \n",
    "            # unsorted order and then sort them will be faster than doing\n",
    "            # straight up argort on the entire score\n",
    "            # http://stackoverflow.com/questions/42184499/cannot-understand-numpy-argpartition-output\n",
    "            ids = np.argpartition(scores, -count)[-count:]\n",
    "            best_ids = np.argsort(scores[ids])[::-1]\n",
    "            best = ids[best_ids]\n",
    "        else:\n",
    "            best = np.argsort(scores)[::-1]\n",
    "\n",
    "        top_n = list(islice((rec for rec in best if rec not in liked), N))\n",
    "        return top_n\n",
    "    \n",
    "    def get_similar_items(self, N = 5, item_ids = None):\n",
    "        \"\"\"\n",
    "        return the top N similar items for itemid, where\n",
    "        cosine distance is used as the distance metric\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        N : int, default 5\n",
    "            top-N similar items' N\n",
    "            \n",
    "        item_ids : 1d iterator, e.g. list or numpy array, default None\n",
    "            the item ids that we wish to find the similar items\n",
    "            of, the default None will compute the similar items\n",
    "            for all the items\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        similar_items : 2d ndarray, shape [number of query item_ids, N]\n",
    "            each row is the top-N most similar item id for each\n",
    "            query item id\n",
    "        \"\"\"\n",
    "        # cosine distance is proportional to normalized euclidean distance,\n",
    "        # thus we normalize the item vectors and use euclidean metric so\n",
    "        # we can use the more efficient kd-tree for nearest neighbor search;\n",
    "        # also the item will always to nearest to itself, so we add 1 to \n",
    "        # get an additional nearest item and remove itself at the end\n",
    "        normed_factors = normalize(self.item_factors)\n",
    "        knn = NearestNeighbors(n_neighbors = N + 1, metric = 'euclidean')\n",
    "        knn.fit(normed_factors)\n",
    "\n",
    "        # returns a distance, index tuple,\n",
    "        # we don't actually need the distance\n",
    "        if item_ids is not None:\n",
    "            normed_factors = normed_factors[item_ids]\n",
    "\n",
    "        _, items = knn.kneighbors(normed_factors)\n",
    "        similar_items = items[:, 1:].astype(np.uint32)\n",
    "        return similar_items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BPR: 100%|██████████| 160/160 [00:05<00:00, 29.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.BPR at 0x7fbd8db00c88>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters were randomly chosen\n",
    "bpr_params = {'reg': 0.01,\n",
    "              'learning_rate': 0.1,\n",
    "              'n_iters': 160,\n",
    "              'n_factors': 30,\n",
    "              'batch_size': 5}\n",
    "\n",
    "bpr = BPR(**bpr_params)\n",
    "bpr.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
